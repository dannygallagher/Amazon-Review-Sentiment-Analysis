{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6c8851261245>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtune\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCLIReporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedulers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mASHAScheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask \n",
    "import dask.dataframe as dd\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import unidecode\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from IPython.display import Image, YouTubeVideo\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting madgrad\n",
      "  Downloading madgrad-1.1-py3-none-any.whl (7.4 kB)\n",
      "Installing collected packages: madgrad\n",
      "Successfully installed madgrad-1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install madgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def HyperTrain(device = device, train_path = None, test_path =None, #file/device params \n",
    "               loader = None, preloaded = False, batch_size = 32, max_vocab= None, embed_vectors = \"glove.6B.100d\", fix_length = None, #loader params\n",
    "               output_size = 2, hidden_layers = 100, embed_size = 100, dropout = 0, #model params\n",
    "               epochs = 10, learning_rate = .001, weight_decay = 0, criterion = nn.CrossEntropyLoss(), optimizer = 'Adam',#training params\n",
    "               choose_best_epoch = True, #testing params\n",
    "               initial_weights = False, #weight initialization\n",
    "               ):\n",
    "    '''\n",
    "  \n",
    "    loader: use to switch to a different loader (if BERT needs a different loader style)\n",
    "    preloaded: default False, set this to True if you already loaded your data and don't want to test any loader configurations\n",
    "    dropout: proportion that should be dropped in dropout layers (0 is equivalent to no dropout)\n",
    "    choose_best_epoch: default True. Set to True if you want the final test evaluation to be done on the model state saved from the epoch with the lowest validation loss\n",
    "    initial_weights: choose if and how you want to initialize weights, can be bool or string of choices\n",
    "    \n",
    "    '''\n",
    "    def load_dataset(batch_size = 32, max_vocab = None, embed_vectors = \"glove.6B.100d\", fix_length = None):\n",
    "        TEXT = data.Field(tokenize = nltk.word_tokenize,\n",
    "                          include_lengths = True, batch_first = True, fix_length = fix_length)\n",
    "        LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "        fields = {'reviewText': ('text', TEXT), 'pos_neg': ('label', LABEL)}\n",
    "        train_data, test_data = data.TabularDataset.splits(\n",
    "            path = '',\n",
    "            train = train_path,\n",
    "            test = test_path,\n",
    "            format = 'json',\n",
    "            fields = fields\n",
    "        )\n",
    "\n",
    "        \n",
    "        #glove example: glove.6B.100d means we use 6 billion 100-dimensional glove embeddings \n",
    "        #use unk_init to set the unembedded words via Gaussian distribution\n",
    "        TEXT.build_vocab(train_data, max_size = max_vocab, vectors = embed_vectors, unk_init = torch.Tensor.normal_)\n",
    "        LABEL.build_vocab(train_data)\n",
    "\n",
    "        train_data, valid_data = train_data.split(split_ratio=0.8, random_state = random.seed(42))\n",
    "\n",
    "        #Use BucketIterator to split batches into reviews of similar length and pad each batch accordingly.\n",
    "        #This will greatly speed up our processing by making us have to process way fewer non-useful pad tokens.\n",
    "        train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                                                       batch_size= batch_size, sort_key=lambda x: len(x.text),\n",
    "                                                                       repeat=False, shuffle=True, sort_within_batch = True)\n",
    "        vocab_size = len(TEXT.vocab)\n",
    "           \n",
    "        #get number of samples for printing          \n",
    "        train_samples = len(train_data)\n",
    "        valid_samples = len(valid_data)\n",
    "        test_samples =len(test_data)\n",
    "        num_samples = train_samples + valid_samples + test_samples\n",
    "\n",
    "        return TEXT, vocab_size, train_iter, valid_iter, test_iter, num_samples\n",
    "    \n",
    "    \n",
    "    ##### PUT ARCHITECTURE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(model, device, train_iter, valid_iter, epochs, learning_rate, criterion = nn.CrossEntropyLoss(), optim = 'Adam'):\n",
    "        if optim == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "        elif optim == 'Madgrad':\n",
    "            optimizer =  MADGRAD(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "        #We'll set up a best validation loss (set to infinity at first) so we can save the best epoch\n",
    "        best_validation_loss = float('inf')\n",
    "        train_loss, validation_loss = [], []\n",
    "        train_acc, validation_acc = [], []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #train\n",
    "            model.train()\n",
    "            running_loss = 0.\n",
    "            correct, total = 0, 0 \n",
    "            steps = 0\n",
    "\n",
    "            for idx, batch in enumerate(train_iter):\n",
    "                text = batch.text[0]\n",
    "              \n",
    "                target = batch.label\n",
    "                target = torch.autograd.Variable(target).long()\n",
    "                text, target = text.to(device), target.to(device)\n",
    "\n",
    "               \n",
    "                optimizer.zero_grad()\n",
    "                output = model(text)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(bert_model.parameters(), HYPER_PARAMS[\"max_grad_norm\"])\n",
    "                optimizer.step()\n",
    "                steps += 1\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # get accuracy \n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "            train_loss.append(running_loss/len(train_iter))\n",
    "            train_acc.append(correct/total)\n",
    "\n",
    "            print(f'Epoch: {epoch + 1},  Training Loss: {running_loss/len(train_iter):.4f}, Training Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "            # evaluate on validation data\n",
    "            model.eval()\n",
    "            running_loss = 0.\n",
    "            correct, total = 0, 0 \n",
    "\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(valid_iter):\n",
    "                    text = batch.text[0]\n",
    "                    target = batch.label\n",
    "                    target = torch.autograd.Variable(target).long()\n",
    "                    text, target = text.to(device), target.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model(text)\n",
    "\n",
    "                    loss = criterion(output, target)\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                    # get accuracy \n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    total += target.size(0)\n",
    "                    correct += (predicted == target).sum().item()\n",
    "\n",
    "            v_loss = running_loss/len(valid_iter)\n",
    "            validation_loss.append(v_loss)\n",
    "            validation_acc.append(correct/total)\n",
    "\n",
    "            print (f'Validation Loss: {v_loss:.4f}, Validation Accuracy: {100*correct/total: .2f}%')\n",
    "            \n",
    "            # If the current epoch has the lowest validation loss, save the model state and use that state for testing\n",
    "            if v_loss < best_validation_loss:\n",
    "                best_validation_loss = v_loss\n",
    "                torch.save(model.state_dict(), 'RNN-train.pt')\n",
    "\n",
    "\n",
    "        return train_loss, train_acc, validation_loss, validation_acc\n",
    "    \n",
    "    \n",
    "    def test(model,  device, test_iter, choose_best_epoch = True):\n",
    "        \n",
    "        #Use epoch with lowest validation loss\n",
    "        if choose_best_epoch:\n",
    "            model.load_state_dict(torch.load('RNN-train.pt'))\n",
    "            \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(test_iter):\n",
    "                text = batch.text[0]\n",
    "                target = batch.label\n",
    "                target = torch.autograd.Variable(target).long()\n",
    "                text, target = text.to(device), target.to(device)\n",
    "\n",
    "                outputs = model(text)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        return acc\n",
    "    \n",
    "    def plot_train_val(x, train, val, train_label, val_label, title):\n",
    "        plt.plot(x, train, label=train_label)\n",
    "        plt.plot(x, val, label=val_label)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def count_parameters(model):\n",
    "        parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return parameters\n",
    "    \n",
    "    # set initial weights\n",
    "    def init_weights(m):\n",
    "        if type(m) in (nn.Linear, nn.Conv1d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            \n",
    "    \n",
    "    # Start the training based on hyperparams\n",
    "    # Initialize model, training and testing\n",
    "    model = architecture(output_size, hidden_size, vocab_size, embedding_length, dropout)\n",
    "    rnn_model.to(device)\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc, validation_loss, validation_acc = train(rnn_model, device, train_iter, valid_iter, epochs, learning_rate)\n",
    "    print(\"--- Training Time = %s seconds ---\" % (time.time() - start_time))\n",
    "    test_accuracy = test(rnn_model, device, test_iter)\n",
    "    print('Test Accuracy: ',  test_accuracy, '%')\n",
    "\n",
    "    # Plot accuracy curves\n",
    "    plot_train_val(np.arange(0,epochs), train_acc, validation_acc,\n",
    "                   'training accuracy', 'validation accuracy', f'{Model_name} on Binary (Positive/Negative), {num_samples} Samples')\n",
    "\n",
    "    # Number of model parameters\n",
    "    paramters = count_parameters(rnn_model)\n",
    "    print('\\n\\nNumber of parameters = ',  paramters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters:\n",
    "\n",
    "\n",
    "    #Loading\n",
    "embed_vectors = [\"glove.6B.100d\"]\n",
    "fix_length = None\n",
    "\n",
    "\n",
    "    # Training \n",
    "learning_rates = [.0005, .001, .005]\n",
    "weight_decay = [0]\n",
    "criterion = [nn.CrossEntropyLoss()]\n",
    "optimizers = ['Adam', 'Madgrad']\n",
    "\n",
    "\n",
    "    # Model\n",
    "output_size = 2\n",
    "hidden_layers = [200] \n",
    "embed_size = [100]\n",
    "epochs = 10\n",
    "dropout = [.3]\n",
    "\n",
    "for e in embed_vectors:\n",
    "    for lr in learning_rates:\n",
    "        for wd in weight_decay:\n",
    "            for c in criterion:\n",
    "                for opt in optimizers:\n",
    "                     for h in hidden_layers:\n",
    "                         for es in embed_size:\n",
    "                             for d in dropout:\n",
    "                                 HyperTrain(embed_vectors = e, fix_length = fix_length, \n",
    "                                            learning_rate = lr, weight_decay = wd, criterion = c, optimizer = opt\n",
    "                                            output_size = output_size, hidden_layers = h, embed_size = es, epochs = epochs, dropout = d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

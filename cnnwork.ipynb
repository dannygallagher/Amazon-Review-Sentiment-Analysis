{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnnwork.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALYtb4z0LzVl"
      },
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h8fE9mfMOkj"
      },
      "source": [
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('Gift_Cards_5.json.gz')\n",
        "df = df[df['reviewText'].notna()]\n",
        "\n",
        "#from spell_check import fixSentence\n",
        "#df['reviewText'] = df['reviewText'].apply(lambda x: fixSentence(x))\n",
        "\n",
        "#print('CHECKPOINT: Spell Check Complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJL09fdZMPyL"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts = cv.fit_transform(df['reviewText'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_counts, df['overall'], test_size = 0.25, random_state = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r0br09yKhkR"
      },
      "source": [
        "\"\"\"\n",
        "Basic CNNModel structure\n",
        "\"\"\"\n",
        "class BasicCNNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__() \n",
        "        self.conv1 = nn.Conv2d(3, 6, 4)\n",
        "        self.conv2 = nn.Conv2d(6, 6, 4)\n",
        "\n",
        "        self.lin1 = nn.Linear(6 * 53 * 53, 6 * 53, bias = True)\n",
        "        self.lin2 = nn.Linear(6 * 53, len(train_dataset.classes), bias = True)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.ReLU = nn.ReLU()\n",
        "      \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 6 * 53 * 53)\n",
        "        x = self.lin1(x)\n",
        "        x = self.ReLU(x)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "basic_cnn_model = BasicCNNModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIIAjDoLjLH"
      },
      "source": [
        "\"\"\"\n",
        "'faster' CNN model (taken from github)\n",
        "Note they used bigrams as inputs, not word 2 vec\n",
        "\"\"\"\n",
        "class FastCNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        \n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        embedded = embedded.permute(1, 0, 2)\n",
        "        \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
        "        \n",
        "        #pooled = [batch size, embedding_dim]\n",
        "                \n",
        "        return self.fc(pooled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sB6XV6dMdPZ"
      },
      "source": [
        "\"\"\"\n",
        "CNN that utilizes dropout (also taken from github)\n",
        "unclear what format they use as inputs for the CNN, look into more\n",
        "\n",
        "author does seem to have some interesting approaches to preprocessing and spellcheck\n",
        "try and test but looks ugly\n",
        "\n",
        "https://github.com/linzehui/pytorch-SentimentAnalysis/blob/master/data.py\n",
        "\"\"\"\n",
        "class DropOutCNN(nn.Module):\n",
        "    def __init__(self, nembedding, vocab_size, kernel_num, kernel_sizes, label_size,\n",
        "                 dropout=0.3, use_pretrain=False, embed_matrix=None, embed_freeze=False):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, nembedding)\n",
        "        if use_pretrain is True:\n",
        "            self.embedding.weight = nn.Parameter(torch.from_numpy(embed_matrix).type(torch.FloatTensor),\n",
        "                                                 requires_grad=not embed_freeze)\n",
        "        self.in_channel = 1\n",
        "        self.out_channel = kernel_num\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.kernel_num = kernel_num\n",
        "        self.convs1 = nn.ModuleList([nn.Conv2d(self.in_channel, self.out_channel, (K, nembedding))\n",
        "                                     for K in self.kernel_sizes])  # kernel_sizes,like (3,4,5)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \"\"\"\n",
        "        in_feature=len(kernel_sizes)*kernel_num,because we concatenate \n",
        "        \"\"\"\n",
        "        self.fc = nn.Linear(len(kernel_sizes) * kernel_num, label_size)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        padded_sentences, lengths = pad_packed_sequence(sequences, padding_value=int(0),\n",
        "                                                        batch_first=True)  # set batch_first true\n",
        "        x = self.embedding(padded_sentences)  # batch_size*num_word*nembedding\n",
        "\n",
        "        x = x.unsqueeze(1)  # (batch_size,1,num_word,nembedding)   1 is in_channel\n",
        "\n",
        "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # a list containing (batch_size,out_channel,W)\n",
        "\n",
        "        x = [F.max_pool1d(e, e.size(2)).squeeze(2) for e in\n",
        "             x]  # max_pool1d(input, kernel_size),now x is a list of (batch_size,out_channel)\n",
        "\n",
        "        x = torch.cat(x, dim=1)  # concatenate , x is batch_size,len(kernel_sizes)*kernel_num\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCzbsXRDK7iM"
      },
      "source": [
        "\"\"\"\n",
        "Basic train loop for cnn\n",
        "\"\"\"\n",
        "def train_cnn(model, optimizer, criterion, epochs=10):\n",
        "    for child in model.children():\n",
        "      if hasattr(child, 'reset_parameters'):\n",
        "        child.reset_parameters()\n",
        "    \n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    for epoch in range(epochs):\n",
        "      losses = []\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        total += labels.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "      epoch_acc = correct / total\n",
        "      epoch_loss = sum(losses) / len(losses)\n",
        "      accuracy_list.append(epoch_acc)\n",
        "      loss_list.append(epoch_loss)\n",
        "      print('Epoch Num: %d, Accuracy: %.4f, Loss: %.4f' % (epoch + 1, epoch_acc, epoch_loss))\n",
        "\n",
        "    final_training_accuracy = accuracy_list[-1]     \n",
        "    final_training_loss = loss_list[-1]\n",
        "    return final_training_loss, final_training_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}